# üîß Fix: Documento Sin Chunks en el √çndice

## üö® Problema Detectado

```
[RAG] B√∫squeda: filter_source='factura-sitio-web-manabivial.pdf', total_chunks=8231, diversify=False
[RAG] Aplicando filtro: 'factura-sitio-web-manabivial.pdf'
[RAG] ‚ö†Ô∏è  Sin resultados para filtro 'factura-sitio-web-manabivial.pdf'
```

### Diagn√≥stico

1. ‚úÖ **Archivo existe f√≠sicamente**: `backend/data/docs/factura-sitio-web-manabivial.pdf`
2. ‚úÖ **Archivo listado en sources**: Aparece en la lista de documentos disponibles
3. ‚ùå **Sin chunks en meta.json**: El archivo NO tiene ning√∫n fragmento procesado
4. ‚ùå **Resultado**: B√∫squedas en ese documento devuelven vac√≠o

---

## üîç An√°lisis T√©cnico

### Estado Actual

```python
# Verificaci√≥n manual:
import json
meta = json.load(open('backend/data/store/meta.json', encoding='utf-8'))

# Contar chunks del archivo
count = sum(1 for s in meta['sources'] if 'factura-sitio-web-manabivial' in s.lower())
print(f"Chunks: {count}")  # Output: 0

# El archivo est√° en la lista √∫nica de fuentes pero sin chunks asociados
unique_sources = set(meta['sources'])
print('factura-sitio-web-manabivial.pdf' in unique_sources)  # True
```

### ¬øPor Qu√© Ocurri√≥?

Posibles causas:

1. **Fallo durante la ingesta inicial**
   - Error al leer/parsear el PDF
   - PDF corrupto o con formato no est√°ndar
   - Error en el chunking (archivo muy peque√±o/vac√≠o)

2. **Problema con PyMuPDF/pypdf**
   - Versi√≥n incompatible
   - L√≠mites de memoria/timeout

3. **Deduplicaci√≥n excesiva**
   - Todos los chunks fueron considerados duplicados
   - Hash collision (muy improbable)

---

## ‚úÖ Soluciones

### Opci√≥n 1: Re-ingestar Solo ese Documento (RECOMENDADO)

```bash
# 1. Eliminar el documento del √≠ndice actual
cd backend
python -c "
import json
from pathlib import Path

meta_path = Path('data/store/meta.json')
meta = json.load(meta_path.open(encoding='utf-8'))

# Filtrar todo lo que NO sea ese documento
target = 'factura-sitio-web-manabivial.pdf'
indices_to_keep = [i for i, s in enumerate(meta['sources']) if s != target]

# Actualizar meta
meta['chunks'] = [meta['chunks'][i] for i in indices_to_keep]
meta['sources'] = [meta['sources'][i] for i in indices_to_keep]
meta['pages'] = [meta['pages'][i] for i in indices_to_keep]

# Guardar
meta_path.write_text(json.dumps(meta, indent=2, ensure_ascii=False), encoding='utf-8')
print(f'Eliminado {target} del √≠ndice')
"

# 2. Re-ingestar solo ese archivo
# Usando la API /rebuild (si existe)
# O eliminando el archivo, reiniciando backend, y subi√©ndolo de nuevo
```

### Opci√≥n 2: Verificar el Contenido del PDF

```bash
# Verificar si el PDF es legible
cd backend
python -c "
import pypdf
from pathlib import Path

pdf_path = Path('data/docs/factura-sitio-web-manabivial.pdf')

try:
    with open(pdf_path, 'rb') as f:
        reader = pypdf.PdfReader(f)
        print(f'P√°ginas: {len(reader.pages)}')
        
        for i, page in enumerate(reader.pages):
            text = page.extract_text()
            print(f'P√°gina {i+1}: {len(text)} caracteres')
            if text:
                print(f'Preview: {text[:200]}...')
except Exception as e:
    print(f'ERROR: {e}')
"
```

### Opci√≥n 3: Re-construir TODO el √çndice (NUCLEAR)

```bash
# ‚ö†Ô∏è CUIDADO: Esto borrar√° todo y re-procesar√° todos los documentos

cd backend

# 1. Backup actual
cp data/store/meta.json data/store/meta.json.backup
cp data/store/faiss.index data/store/faiss.index.backup

# 2. Eliminar √≠ndice actual
rm data/store/meta.json
rm data/store/faiss.index

# 3. Re-ingestar todos los documentos
# M√©todo A: Usar endpoint /rebuild
curl -X POST http://localhost:8000/rebuild

# M√©todo B: Usar script de ingesta
python -c "
from ingest import build_or_update_index
from pathlib import Path
from config import DOCS_DIR

docs = list(DOCS_DIR.glob('*.pdf')) + list(DOCS_DIR.glob('*.txt')) + list(DOCS_DIR.glob('*.md'))
build_or_update_index([str(d) for d in docs])
print('√çndice reconstruido')
"
```

---

## üõ†Ô∏è Soluci√≥n R√°pida (FRONTEND)

### Si Solo Necesitas el Archivo Espec√≠fico

**Opci√≥n 1: Re-subir el archivo**

1. Descargar el PDF:
   ```bash
   # Hacer copia de seguridad
   cp backend/data/docs/factura-sitio-web-manabivial.pdf ~/Desktop/
   ```

2. En el frontend, usar el bot√≥n de "Upload" para subir el archivo de nuevo

3. El sistema deber√≠a:
   - Detectar que el archivo ya existe
   - Re-procesarlo completamente
   - Generar chunks nuevos

**Opci√≥n 2: Usar otro documento temporalmente**

Mientras se soluciona el problema, el usuario puede:
- Buscar en "Todos los documentos" (b√∫squeda global)
- El RAG encontrar√° informaci√≥n similar en otros documentos

---

## üîß Implementaci√≥n del Fix

### Script de Diagn√≥stico

```python
# backend/scripts/check_missing_chunks.py
import json
from pathlib import Path
from collections import Counter

meta_path = Path('backend/data/store/meta.json')
meta = json.load(meta_path.open(encoding='utf-8'))

# Contar chunks por documento
chunk_counts = Counter(meta['sources'])

print("üìä Chunks por documento:")
print("=" * 60)

for source in sorted(chunk_counts.keys()):
    count = chunk_counts[source]
    status = "‚úÖ" if count > 0 else "‚ùå SIN CHUNKS"
    print(f"{status} {source}: {count} chunks")

# Documentos sin chunks
no_chunks = [s for s, c in chunk_counts.items() if c == 0]
if no_chunks:
    print(f"\nüö® Documentos SIN chunks: {len(no_chunks)}")
    for doc in no_chunks:
        print(f"  - {doc}")
else:
    print("\n‚úÖ Todos los documentos tienen chunks")
```

### Script de Reparaci√≥n

```python
# backend/scripts/fix_document.py
import sys
from pathlib import Path
from ingest import build_or_update_index
from config import DOCS_DIR

def fix_document(filename: str):
    """Re-ingesta un documento espec√≠fico."""
    doc_path = DOCS_DIR / filename
    
    if not doc_path.exists():
        print(f"‚ùå Archivo no encontrado: {doc_path}")
        return
    
    print(f"üîß Re-ingesting: {filename}")
    
    # Eliminar del √≠ndice actual (opcional)
    # TODO: implementar eliminaci√≥n selectiva
    
    # Re-ingestar
    try:
        build_or_update_index([str(doc_path)])
        print(f"‚úÖ {filename} re-ingested successfully")
    except Exception as e:
        print(f"‚ùå Error: {e}")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python fix_document.py <filename>")
        sys.exit(1)
    
    fix_document(sys.argv[1])
```

---

## üìã Plan de Acci√≥n

### Inmediato (para el usuario)

1. **Explicar el problema**:
   ```
   El documento "factura-sitio-web-manabivial.pdf" no se proces√≥ correctamente 
   durante la ingesta inicial. Existe f√≠sicamente pero no tiene fragmentos 
   indexados en el sistema RAG.
   ```

2. **Soluci√≥n temporal**:
   ```
   Por ahora, usa "Buscar en todos los documentos" (sin seleccionar archivo 
   espec√≠fico) para buscar informaci√≥n relacionada en todo el corpus.
   ```

3. **Soluci√≥n permanente**:
   ```
   Re-sube el documento usando el bot√≥n de Upload en el frontend. 
   El sistema lo re-procesar√° autom√°ticamente.
   ```

### A Mediano Plazo (desarrollo)

1. ‚úÖ **Agregar validaci√≥n en ingesta**
   ```python
   # En ingest.py, despu√©s de procesar:
   if not chunks_for_doc:
       logger.warning(f"‚ö†Ô∏è Document {filename} produced 0 chunks!")
   ```

2. ‚úÖ **Health check endpoint**
   ```python
   @app.get("/health/documents")
   def check_documents_health():
       """Retorna documentos sin chunks o con problemas."""
       index, chunks, sources, pages = ingest.load_index_safe()
       
       from collections import Counter
       counts = Counter(sources)
       
       no_chunks = [s for s, c in counts.items() if c == 0]
       low_chunks = [s for s, c in counts.items() if 0 < c < 5]
       
       return {
           "no_chunks": no_chunks,
           "low_chunks": low_chunks,
           "total_docs": len(counts),
           "total_chunks": len(chunks)
       }
   ```

3. ‚úÖ **UI para re-procesar documentos**
   ```tsx
   // En frontend, bot√≥n para re-ingestar documento espec√≠fico
   <button onClick={() => reprocessDocument(doc.name)}>
     üîÑ Re-procesar
   </button>
   ```

---

## üéØ Resultado Esperado

Despu√©s del fix:

```
[RAG] B√∫squeda: filter_source='factura-sitio-web-manabivial.pdf', total_chunks=8231, diversify=False
[RAG] Aplicando filtro: 'factura-sitio-web-manabivial.pdf'
[RAG] Filtro aplicado: 342 ‚Üí 15 chunks  ‚úÖ
```

El documento ahora tiene chunks y las b√∫squedas funcionan correctamente.

---

## üìö Referencias

- `backend/ingest.py` - Funci√≥n `build_or_update_index()` l√≠nea ~230
- `backend/rag.py` - Funci√≥n `search()` con filtro l√≠nea ~249-263
- `backend/data/store/meta.json` - Estructura de metadatos

---

## ‚ö†Ô∏è Prevenci√≥n Futura

### Mejoras al Proceso de Ingesta

1. **Logging detallado por archivo**
   ```python
   logger.info(f"Processing {filename}...")
   logger.info(f"  - Extracted {len(pages)} pages")
   logger.info(f"  - Generated {len(chunks)} chunks")
   logger.info(f"  - After dedup: {len(final_chunks)} chunks")
   ```

2. **Validaci√≥n post-ingesta**
   ```python
   if len(final_chunks) == 0:
       logger.error(f"‚ùå ZERO chunks for {filename}!")
       # Opcionalmente: raise o notificar
   ```

3. **Reporte de salud autom√°tico**
   ```python
   # Al finalizar ingesta completa
   print("\nüìä Ingestion Summary:")
   for filename in processed_files:
       count = sum(1 for s in sources if s == filename)
       print(f"  - {filename}: {count} chunks")
   ```

---

**√öltima actualizaci√≥n**: 2025-10-11  
**Estado**: Documentado, pendiente de fix
